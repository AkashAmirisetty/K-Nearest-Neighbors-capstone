# K-Nearest Neighbors Capstone Project

This project demonstrates how to implement and apply the **K-Nearest Neighbors (KNN)** algorithm using Python and real-world data. It is part of a machine learning capstone exercise, showcasing data preprocessing, model training, and evaluation techniques.

## ğŸ“ Project Structure
```
K-Nearest-Neighbors-capstone-main/
â”‚
â”œâ”€â”€ K Nearest Neighbors Project.ipynb # Main implementation notebook
â”œâ”€â”€ K-Nearest-Neighbors Practice.ipynb # Practice notebook
â”œâ”€â”€ Classified Data/ # Supplementary dataset folder
â”œâ”€â”€ KNN_Project_Data/ # Primary dataset folder
â””â”€â”€ .ipynb_checkpoints/ # Auto-saved notebook states
```

## ğŸ“Œ What I Learnt from this

- How to load and explore data for classification
- Applying feature scaling
- Using the KNN algorithm for classification tasks
- Evaluating model performance using confusion matrix & classification report
- Best practices for choosing the value of *K*

## ğŸ§ª Requirements

- Python 3.x
- Jupyter Notebook
- pandas
- numpy
- seaborn
- matplotlib
- scikit-learn

## ğŸ“Š Sample Outputs
Confusion Matrix
Classification Report
Accuracy and error plots by K value
